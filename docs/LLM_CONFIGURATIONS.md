# LLM Configurations Used During Development

The pipeline was tested with **three configurations** by changing only `config.py` (and `.env`), without modifying the rest of the code.

---

## 1. Single local model (default)

- **Idea:** One local model (e.g. Qwen2.5-Coder or a general instruct model) for both SPARQL generation and French answer generation.
- **Config:**
  - `USE_LOCAL_LLM=true`
  - `LOCAL_LLM_ENDPOINT=http://localhost:1234/v1` (or your LM Studio / Ollama-compatible endpoint)
  - `LOCAL_LLM_MODEL=Qwen2.5-Coder-14B-Instruct` (or any single model name)
  - Leave `SPARQL_LLM_MODEL` and `ANSWER_LLM_MODEL` **empty**
- **Behavior:** `llm_client.get_sparql_llm()` and `get_answer_llm()` both use `LOCAL_LLM_MODEL` with different temperatures (0 for SPARQL, 0.3 for answers) and system prompts.
- **Use case:** Quick setup, single model to run locally.

---

## 2. Dual specialized local models (recommended for quality)

- **Idea:** One model good at **code/SPARQL** (e.g. Qwen2.5-Coder) and one good at **French language** (e.g. Mistral, Vigogne, Llama).
- **Config:**
  - `USE_LOCAL_LLM=true`
  - `LOCAL_LLM_ENDPOINT=http://localhost:1234/v1`
  - `LOCAL_LLM_MODEL=Qwen2.5-Coder-14B-Instruct` (fallback if specialized are not set)
  - `SPARQL_LLM_MODEL=qwen2.5-coder-14b-instruct-mlx` (or your code model name in the server)
  - `ANSWER_LLM_MODEL=meta-llama-3.1-8b-instruct` (or your French/instruction model name)
- **Behavior:** SPARQL is generated by `SPARQL_LLM_MODEL` (low temperature), answers by `ANSWER_LLM_MODEL` (slightly higher temperature). Same endpoint, two different model names.
- **Use case:** Best balance of SPARQL accuracy and natural French answers; used for the evaluation run documented in [EVALUATION_RESULTS.md](EVALUATION_RESULTS.md).

---

## 3. OpenAI (cloud)

- **Idea:** Use OpenAI for one or both steps (SPARQL and/or answer). The codebase is written for a local OpenAI-compatible API; switching to real OpenAI would require a thin adapter in `llm_client.py` that calls `openai` with `OPENAI_API_KEY` and `OPENAI_MODEL` instead of the local endpoint.
- **Config (conceptual):**
  - `USE_LOCAL_LLM=false` (and in code, branch on this to call OpenAI)
  - `OPENAI_API_KEY=sk-...`
  - `OPENAI_MODEL=gpt-4` (or gpt-4o-mini for cost)
- **Note:** The current `llm_client.py` is built around a single local endpoint. To fully use “configuration 3”, you would add an `OpenAILLMClient` and wire it in when `USE_LOCAL_LLM=false`. The **configuration pattern** (single vs dual model, temperature, system prompts) stays the same; only the HTTP client and endpoint change.

---

## Summary

| Config              | SPARQL model     | Answer model     | Where set                    |
|---------------------|------------------|------------------|------------------------------|
| Single local        | `LOCAL_LLM_MODEL`| `LOCAL_LLM_MODEL`| `.env` / `config.py`         |
| Dual specialized    | `SPARQL_LLM_MODEL` | `ANSWER_LLM_MODEL` | `.env` / `config.py`       |
| OpenAI (cloud)      | `OPENAI_MODEL`   | `OPENAI_MODEL`   | Requires adapter in `llm_client` |

Validation: run `python config.py` from the `code/` directory; it prints the active configuration and checks that required variables are set.
