"""
ğŸ¯ Run Semantic GraphRAG Evaluation
================================

Evaluates answers using semantic similarity and LLM-as-judge
Perfect for when answers are correct but rephrased differently
"""

import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path

# Add code/ to path (parent of evaluation/)
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from intelligent_chatbot import IntelligentEquestrianChatbot
from evaluation_service import (
    init_evaluator,
    calculate_semantic_similarity,
    llm_judge_answer,
    COST_PER_1K_INPUT,
    COST_PER_1K_OUTPUT,
    COST_PER_1K_EMBEDDING,
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Configuration
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_SCRIPT_DIR = Path(__file__).resolve().parent
_CODE_DIR = _SCRIPT_DIR.parent
_PROJECT_ROOT = _CODE_DIR.parent

TEST_DATASET_PATH = _SCRIPT_DIR / "test_dataset.json"
if not TEST_DATASET_PATH.exists():
    TEST_DATASET_PATH = _PROJECT_ROOT / "data" / "test_dataset.json"
RESULTS_DIR = _PROJECT_ROOT / "evaluation_results"
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

print("="*80)
print("ğŸ¯ SEMANTIC GRAPHRAG EVALUATION")
print("="*80)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Initialize evaluator LLM and embeddings
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸ¤– Initializing evaluation tools...")
judge_llm, embeddings = init_evaluator()
print("âœ… Evaluation tools ready")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Load test dataset
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸ“¥ Loading test dataset...")
if not TEST_DATASET_PATH.exists():
    print(f"âŒ Test dataset not found: {TEST_DATASET_PATH}")
    print("   Create data/test_dataset.json or code/evaluation/test_dataset.json")
    print('   Format: {"test_questions": [{"question_id", "question", "ground_truth", "category", "difficulty"}, ...]}')
    sys.exit(1)
with open(TEST_DATASET_PATH, 'r', encoding='utf-8') as f:
    test_data = json.load(f)
questions_data = test_data['test_questions']
print(f"âœ… Loaded {len(questions_data)} test questions")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Initialize GraphRAG (IntelligentEquestrianChatbot)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸ”„ Initializing GraphRAG system...")
chatbot = IntelligentEquestrianChatbot()
print("âœ… GraphRAG system initialized")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Run evaluation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸš€ Running evaluation with semantic analysis...")
print("â”€" * 80)

results = []
total_time = 0
total_query_cost = 0
total_eval_cost = 0

for i, q_data in enumerate(questions_data, 1):
    question_id = q_data['question_id']
    question = q_data['question']
    ground_truth = q_data['ground_truth']
    category = q_data['category']
    difficulty = q_data['difficulty']

    print(f"\n[{i}/{len(questions_data)}] {question_id} ({difficulty}) - {category}")
    print(f"Q: {question[:80]}...")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step 1: Get answer from GraphRAG
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    start_time = time.time()

    try:
        result = chatbot.answer_question(question, verbose=False)
        answer = result.get("answer", "") if result.get("success") else ""
        sparql_query = result.get("sparql_query", "")

        query_time = time.time() - start_time

        # Estimate query cost
        query_tokens = len(question.split()) * 1.3 + 1000 + len(answer.split()) * 1.3
        query_cost = query_tokens / 1000 * COST_PER_1K_INPUT
        total_query_cost += query_cost

        success = result.get("success", False) and len(answer) > 0 and 'error' not in answer.lower()

    except Exception as e:
        answer = f"ERROR: {str(e)}"
        sparql_query = ""
        query_time = time.time() - start_time
        success = False
        query_cost = 0

    total_time += query_time

    print(f"âœ… Query answered in {query_time:.2f}s")
    print(f"A: {answer[:100]}...")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step 2: Evaluate answer quality
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    if success:
        print("ğŸ“Š Evaluating answer quality...")

        eval_start = time.time()

        # Semantic similarity using embeddings
        semantic_score = calculate_semantic_similarity(answer, ground_truth, embeddings)

        # LLM-as-judge evaluation
        judge_scores = llm_judge_answer(question, answer, ground_truth, judge_llm)

        eval_time = time.time() - eval_start

        # Estimate evaluation cost
        eval_tokens = len(question.split() + answer.split() + ground_truth.split()) * 2
        eval_cost = (eval_tokens / 1000 * COST_PER_1K_INPUT +
                    200 / 1000 * COST_PER_1K_OUTPUT +  # Judge response
                    (len(answer) + len(ground_truth)) / 4 * COST_PER_1K_EMBEDDING)  # Embeddings
        total_eval_cost += eval_cost

        print(f"   Semantic Similarity: {semantic_score:.2f}")
        print(f"   LLM Judge Overall: {judge_scores['overall']:.2f}")
        print(f"   Evaluation time: {eval_time:.2f}s")

    else:
        semantic_score = 0.0
        judge_scores = {
            'correctness': 0.0,
            'completeness': 0.0,
            'accuracy': 0.0,
            'overall': 0.0,
            'reasoning': 'Query failed'
        }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Store results
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    results.append({
        'question_id': question_id,
        'question': question,
        'answer': answer,
        'ground_truth': ground_truth,
        'category': category,
        'difficulty': difficulty,
        'time_seconds': query_time,
        'sparql_query': sparql_query,
        'success': success,
        'semantic_similarity': semantic_score,
        'llm_judge_scores': judge_scores,
        'combined_score': (semantic_score + judge_scores['overall']) / 2  # Average
    })

print("\n" + "="*80)
print("âœ… Evaluation completed!")
print(f"â±ï¸  Total query time: {total_time:.2f}s")
print(f"â±ï¸  Avg query time: {total_time/len(questions_data):.2f}s")
print(f"ğŸ’° Query cost: ${total_query_cost:.4f}")
print(f"ğŸ’° Evaluation cost: ${total_eval_cost:.4f}")
print(f"ğŸ’° Total cost: ${total_query_cost + total_eval_cost:.4f}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Generate report
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸ“Š Generating report...")

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
report_file = RESULTS_DIR / f"semantic_evaluation_{timestamp}.json"

# Calculate statistics by category
category_stats = {}
for category in set(r['category'] for r in results):
    cat_results = [r for r in results if r['category'] == category]
    category_stats[category] = {
        'count': len(cat_results),
        'success_rate': sum(1 for r in cat_results if r['success']) / len(cat_results),
        'avg_time': sum(r['time_seconds'] for r in cat_results) / len(cat_results),
        'avg_semantic_similarity': sum(r['semantic_similarity'] for r in cat_results) / len(cat_results),
        'avg_llm_judge': sum(r['llm_judge_scores']['overall'] for r in cat_results) / len(cat_results),
        'avg_combined': sum(r['combined_score'] for r in cat_results) / len(cat_results),
    }

# Calculate statistics by difficulty
difficulty_stats = {}
for difficulty in set(r['difficulty'] for r in results):
    diff_results = [r for r in results if r['difficulty'] == difficulty]
    difficulty_stats[difficulty] = {
        'count': len(diff_results),
        'success_rate': sum(1 for r in diff_results if r['success']) / len(diff_results),
        'avg_time': sum(r['time_seconds'] for r in diff_results) / len(diff_results),
        'avg_semantic_similarity': sum(r['semantic_similarity'] for r in diff_results) / len(diff_results),
        'avg_llm_judge': sum(r['llm_judge_scores']['overall'] for r in diff_results) / len(diff_results),
    }

# Full report
report = {
    'metadata': {
        'timestamp': timestamp,
        'total_questions': len(results),
        'total_time_seconds': total_time,
        'avg_time_per_question': total_time / len(results),
        'query_cost_usd': total_query_cost,
        'evaluation_cost_usd': total_eval_cost,
        'total_cost_usd': total_query_cost + total_eval_cost,
        'model': 'gpt-4o-mini (judge) + local SPARQL/Answer LLMs',
        'evaluation_type': 'semantic_similarity + llm_judge'
    },
    'overall_metrics': {
        'success_rate': sum(1 for r in results if r['success']) / len(results),
        'failed_count': sum(1 for r in results if not r['success']),
        'avg_semantic_similarity': sum(r['semantic_similarity'] for r in results) / len(results),
        'avg_llm_judge_overall': sum(r['llm_judge_scores']['overall'] for r in results) / len(results),
        'avg_llm_judge_correctness': sum(r['llm_judge_scores']['correctness'] for r in results) / len(results),
        'avg_llm_judge_completeness': sum(r['llm_judge_scores']['completeness'] for r in results) / len(results),
        'avg_llm_judge_accuracy': sum(r['llm_judge_scores']['accuracy'] for r in results) / len(results),
        'avg_combined_score': sum(r['combined_score'] for r in results) / len(results),
    },
    'category_stats': category_stats,
    'difficulty_stats': difficulty_stats,
    'detailed_results': results
}

with open(report_file, 'w', encoding='utf-8') as f:
    json.dump(report, f, ensure_ascii=False, indent=2)

print(f"âœ… Report saved to: {report_file}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Print summary
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("ğŸ“Š SEMANTIC EVALUATION SUMMARY")
print("="*80)

print(f"\nğŸ“ˆ Overall Metrics:")
print(f"  Success Rate: {report['overall_metrics']['success_rate']*100:.1f}%")
print(f"  Failed Questions: {report['overall_metrics']['failed_count']}")
print(f"  Avg Time: {report['metadata']['avg_time_per_question']:.2f}s")
print(f"  Total Cost: ${report['metadata']['total_cost_usd']:.4f}")

print(f"\nğŸ¯ Quality Scores (0-1 scale):")
print(f"  Semantic Similarity: {report['overall_metrics']['avg_semantic_similarity']:.3f}")
print(f"  LLM Judge Overall: {report['overall_metrics']['avg_llm_judge_overall']:.3f}")
print(f"  LLM Judge Correctness: {report['overall_metrics']['avg_llm_judge_correctness']:.3f}")
print(f"  LLM Judge Completeness: {report['overall_metrics']['avg_llm_judge_completeness']:.3f}")
print(f"  LLM Judge Accuracy: {report['overall_metrics']['avg_llm_judge_accuracy']:.3f}")
print(f"  Combined Score: {report['overall_metrics']['avg_combined_score']:.3f}")

print(f"\nğŸ“Š By Category:")
for category, stats in sorted(category_stats.items(), key=lambda x: x[1]['avg_combined'], reverse=True):
    print(f"  {category}:")
    print(f"    Success: {stats['success_rate']*100:.1f}%")
    print(f"    Semantic: {stats['avg_semantic_similarity']:.2f}")
    print(f"    LLM Judge: {stats['avg_llm_judge']:.2f}")
    print(f"    Combined: {stats['avg_combined']:.2f}")

print(f"\nğŸ“Š By Difficulty:")
for difficulty, stats in sorted(difficulty_stats.items()):
    print(f"  {difficulty}:")
    print(f"    Success: {stats['success_rate']*100:.1f}%")
    print(f"    Semantic: {stats['avg_semantic_similarity']:.2f}")
    print(f"    LLM Judge: {stats['avg_llm_judge']:.2f}")

# Show some high and low scoring examples
print(f"\nâœ… Best Answers (Top 3):")
best = sorted([r for r in results if r['success']], key=lambda x: x['combined_score'], reverse=True)[:3]
for r in best:
    print(f"  {r['question_id']}: {r['question'][:60]}...")
    print(f"    Combined Score: {r['combined_score']:.2f}")

print(f"\nâš ï¸  Worst Answers (Bottom 3):")
worst = sorted([r for r in results if r['success']], key=lambda x: x['combined_score'])[:3]
for r in worst:
    print(f"  {r['question_id']}: {r['question'][:60]}...")
    print(f"    Combined Score: {r['combined_score']:.2f}")
    print(f"    Reason: {r['llm_judge_scores']['reasoning'][:80]}...")

print("\n" + "="*80)
print("âœ… Semantic evaluation complete!")
print("="*80)
